{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-S5Q0Ja1qlxM"
      },
      "source": [
        "# 필요한 라이브러리 import\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QCNgkA4DJETe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pz192HxKqlSK"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40vlolhCqqdv"
      },
      "source": [
        "# 유틸리티 함수 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbQifXuxqs5d"
      },
      "source": [
        "# input 데이터와 input 데이터를 한글자씩 뒤로 민 target 데이터를 생성하는 utility 함수를 정의합니다.\n",
        "def split_input_target(chunk):\n",
        "  input_text = chunk[:-1]\n",
        "  target_text = chunk[1:]\n",
        "\n",
        "  return input_text, target_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrJBc9ujquQ4"
      },
      "source": [
        "# 설정값 지정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ap0uWvq9qvkH",
        "outputId": "027a78d7-dd56-4fff-e204-5ebc7837dad4"
      },
      "source": [
        "# 학습에 필요한 설정값들을 지정합니다.\n",
        "data_dir = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')  # shakespeare\n",
        "batch_size = 64      # Training : 64, Sampling : 1\n",
        "seq_length = 100     # Training : 100, Sampling : 1\n",
        "embedding_dim = 256  # Embedding 차원\n",
        "hidden_size = 1024   # 히든 레이어의 노드 개수\n",
        "num_epochs = 10"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vGb_BMhqy8m"
      },
      "source": [
        "# 어휘 집합(Vocabulary set) 설정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEgzFH5Bq2AJ",
        "outputId": "76272136-fc11-457b-a873-d5624fffe2eb"
      },
      "source": [
        "# 학습에 사용할 txt 파일을 읽습니다.\n",
        "text = open(data_dir, 'rb').read().decode(encoding='utf-8')\n",
        "# 학습데이터에 포함된 모든 character들을 나타내는 변수인 vocab과\n",
        "# vocab에 id를 부여해 dict 형태로 만든 char2idx를 선언합니다.\n",
        "vocab = sorted(set(text))  # 유니크한 character 개수\n",
        "vocab_size = len(vocab)\n",
        "print('{} unique characters'.format(vocab_size))\n",
        "char2idx = {u: i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "65 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMkdoIeAq5DJ"
      },
      "source": [
        "# Dataset 설정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N57em_CFrAwG"
      },
      "source": [
        "# 학습 데이터를 character에서 integer로 변환합니다.\n",
        "text_as_int = np.array([char2idx[c] for c in text])\n",
        "\n",
        "# split_input_target 함수를 이용해서 input 데이터와 input 데이터를 한글자씩 뒤로 민 target 데이터를 생성합니다.\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "# tf.data API를 이용해서 데이터를 섞고 batch 형태로 가져옵니다.\n",
        "dataset = dataset.shuffle(10000).batch(batch_size, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGraOEVirC1w"
      },
      "source": [
        "# RNN 모델 설정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgUcylhQrFgX"
      },
      "source": [
        "# tf.keras.Model을 이용해서 RNN 모델을 정의합니다.\n",
        "class RNN(tf.keras.Model):\n",
        " def __init__(self, batch_size):\n",
        "   super(RNN, self).__init__()\n",
        "   self.embedding_layer = tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                                                    batch_input_shape=[batch_size, None])\n",
        "   self.hidden_layer_1 = tf.keras.layers.LSTM(hidden_size,\n",
        "                                             return_sequences=True,\n",
        "                                             stateful=True,\n",
        "                                             recurrent_initializer='glorot_uniform')\n",
        "   self.output_layer = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        " def call(self, x):\n",
        "   embedded_input = self.embedding_layer(x)\n",
        "   features = self.hidden_layer_1(embedded_input)\n",
        "   logits = self.output_layer(features)\n",
        "\n",
        "   return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdZlqa1SrGjL"
      },
      "source": [
        "# Loss Function 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzTsknETrJQC"
      },
      "source": [
        "# sparse cross-entropy 손실 함수를 정의합니다.\n",
        "def sparse_cross_entropy_loss(labels, logits):\n",
        "  return tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7_1vbG0rMW9"
      },
      "source": [
        "# 옵티마이저 및 학습 설정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DMYAASYrOQi"
      },
      "source": [
        "# 최적화를 위한 Adam 옵티마이저를 정의합니다.\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "# 최적화를 위한 function을 정의합니다.\n",
        "@tf.function\n",
        "def train_step(model, input, target):\n",
        "  with tf.GradientTape() as tape:\n",
        "    logits = model(input)\n",
        "    loss = sparse_cross_entropy_loss(target, logits)\n",
        "  grads = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "  return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3E2PRvnrSeP"
      },
      "source": [
        "# 샘플링 함수 설정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgeX0b6HrRyA"
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  num_sampling = 4000  # 생성할 글자(Character)의 개수를 지정합니다.\n",
        "\n",
        "  # start_sting을 integer 형태로 변환합니다.\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # 샘플링 결과로 생성된 string을 저장할 배열을 초기화합니다.\n",
        "  text_generated = []\n",
        "\n",
        "  # 낮은 temperature 값은 더욱 정확한 텍스트를 생성합니다.\n",
        "  # 높은 temperature 값은 더욱 다양한 텍스트를 생성합니다.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # 여기서 batch size = 1 입니다.\n",
        "  model.reset_states()\n",
        "  for i in range(num_sampling):\n",
        "    predictions = model(input_eval)\n",
        "    # 불필요한 batch dimension을 삭제합니다.\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "    # 모델의 예측결과에 기반해서 랜덤 샘플링을 하기위해 categorical distribution을 사용합니다.\n",
        "    predictions = predictions / temperature\n",
        "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "    # 예측된 character를 다음 input으로 사용합니다.\n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\n",
        "    # 샘플링 결과를 text_generated 배열에 추가합니다.\n",
        "    text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2krRKnbvr00T"
      },
      "source": [
        "# 트레이닝 시작"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYxCGdZvoYHP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e6933e3-d2ea-4465-ab59-056247fe237c"
      },
      "source": [
        "# Recurrent Neural Networks(RNN) 모델을 선언합니다.\n",
        "RNN_model = RNN(batch_size=batch_size)\n",
        "\n",
        "# 데이터 구조 파악을 위해서 예제로 임의의 하나의 배치 데이터 에측하고, 예측결과를 출력합니다.\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = RNN_model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
        "\n",
        "# 모델 정보를 출력합니다.\n",
        "RNN_model.summary()\n",
        "\n",
        "# checkpoint 데이터를 저장할 경로를 지정합니다.\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  start = time.time()\n",
        "\n",
        "  # 매 반복마다 hidden state를 초기화합니다. (최초의 hidden 값은 None입니다.)\n",
        "  hidden = RNN_model.reset_states()\n",
        "\n",
        "  for (batch_n, (input, target)) in enumerate(dataset):\n",
        "    loss = train_step(RNN_model, input, target)\n",
        "\n",
        "    if batch_n % 100 == 0:\n",
        "      template = 'Epoch {} Batch {} Loss {}'\n",
        "      print(template.format(epoch+1, batch_n, loss))\n",
        "\n",
        "  # 5회 반복마다 파라미터를 checkpoint로 저장합니다.\n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    RNN_model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "  print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
        "  print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "\n",
        "RNN_model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "print(\"트레이닝이 끝났습니다!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n",
            "Model: \"rnn\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        multiple                  16640     \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  multiple                  5246976   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                multiple                  66625     \n",
            "=================================================================\n",
            "Total params: 5,330,241\n",
            "Trainable params: 5,330,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1 Batch 0 Loss 4.175055027008057\n",
            "Epoch 1 Batch 100 Loss 2.431004285812378\n",
            "Epoch 1 Loss 2.1752\n",
            "Time taken for 1 epoch 13.560356616973877 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 2.7635302543640137\n",
            "Epoch 2 Batch 100 Loss 2.023911237716675\n",
            "Epoch 2 Loss 1.8483\n",
            "Time taken for 1 epoch 11.798667907714844 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.8558545112609863\n",
            "Epoch 3 Batch 100 Loss 1.7246994972229004\n",
            "Epoch 3 Loss 1.7123\n",
            "Time taken for 1 epoch 12.044892072677612 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.6687774658203125\n",
            "Epoch 4 Batch 100 Loss 1.5581393241882324\n",
            "Epoch 4 Loss 1.5780\n",
            "Time taken for 1 epoch 12.164169549942017 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.5290617942810059\n",
            "Epoch 5 Batch 100 Loss 1.5198196172714233\n",
            "Epoch 5 Loss 1.4859\n",
            "Time taken for 1 epoch 12.467072248458862 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 1.4523414373397827\n",
            "Epoch 6 Batch 100 Loss 1.4488029479980469\n",
            "Epoch 6 Loss 1.4593\n",
            "Time taken for 1 epoch 12.806703567504883 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 1.4101111888885498\n",
            "Epoch 7 Batch 100 Loss 1.4568243026733398\n",
            "Epoch 7 Loss 1.3941\n",
            "Time taken for 1 epoch 13.12714171409607 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 1.343491792678833\n",
            "Epoch 8 Batch 100 Loss 1.4062541723251343\n",
            "Epoch 8 Loss 1.3746\n",
            "Time taken for 1 epoch 13.026273727416992 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 1.324224829673767\n",
            "Epoch 9 Batch 100 Loss 1.3243869543075562\n",
            "Epoch 9 Loss 1.3649\n",
            "Time taken for 1 epoch 12.743422269821167 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 1.3133295774459839\n",
            "Epoch 10 Batch 100 Loss 1.297133445739746\n",
            "Epoch 10 Loss 1.3381\n",
            "Time taken for 1 epoch 12.610009670257568 sec\n",
            "\n",
            "트레이닝이 끝났습니다!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Un7SxSy9r6j0"
      },
      "source": [
        "# 트레이닝이 끝난 모델을 이용한 샘플링"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vL5aDi4Ioaz8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "693badd4-d09e-494e-cebc-6545773f0b6a"
      },
      "source": [
        "sampling_RNN_model = RNN(batch_size=1)\n",
        "sampling_RNN_model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "sampling_RNN_model.build(tf.TensorShape([1, None]))\n",
        "sampling_RNN_model.summary()\n",
        "\n",
        "# 샘플링을 시작합니다.\n",
        "print(\"샘플링을 시작합니다!\")\n",
        "print(generate_text(sampling_RNN_model, start_string=u' '))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"rnn_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      multiple                  16640     \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                multiple                  5246976   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              multiple                  66625     \n",
            "=================================================================\n",
            "Total params: 5,330,241\n",
            "Trainable params: 5,330,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "샘플링을 시작합니다!\n",
            " PARENT:\n",
            "And will e wo; 'tis to dark foldou to see't and ignorant.\n",
            "Both seem you of me; blessand is this tender-vern.\n",
            "Caliland, beak it.\n",
            "\n",
            "CORIOLANUS:\n",
            "Towards my sors we flow: but I can not been in speer?\n",
            "\n",
            "Second Gentleman:\n",
            "Therefore, then my breasts till of weary richer hath,\n",
            "Madies on the viceous earrnes with his\n",
            "jest, at your birst-palt'd givings, holds on him;\n",
            "And that some 'briar' in the world's wit.\n",
            "I think another cels, there that blood;\n",
            "But so to 'twas match'd a nobles crost\n",
            "The extrempts of sucr\n",
            "finger the seam cordse would; and thou must not, before\n",
            "My nameop we were fear, the field appresent law,\n",
            "The sick o' the more of the other moon\n",
            "And love' dinst thou livest, sun; leavith him than it.'\n",
            "Did, and leave this palmern service, looks so,\n",
            "For it is a greace: say I so the sameth'd in me,\n",
            "Untiting and all whereto me o'er al I know\n",
            "For the Boriolio Kateingrous in yourself,\n",
            "He alters with her pastifore;\n",
            "If Kath affection shines these words; we 'twixt my\n",
            "but it, Onceivery, thou noigistes\n",
            "Upon them to death with serves.\n",
            "\n",
            "ROMEO:\n",
            "Thus for the thy house, she comes awaked me.\n",
            "Come, losa,\n",
            "Then swearing hath no this innocent shall procea,\n",
            "For thou dispituous air, let us go.\n",
            "\n",
            "HERMIONE:\n",
            "Methinks, this wild never showed bend.\n",
            "\n",
            "BIONTERS:\n",
            "Then now my soul's mother, breathes to wound him\n",
            "As by your sibs midst not, sweet untone:\n",
            "To-stier my blood and aspittant of of\n",
            "Was thus have ever affects I begin you,\n",
            "And lips with oships that swanness,\n",
            "Sucance's that nothing turn till their thing is thee:\n",
            "When thou reck'd with the varse there.\n",
            "\n",
            "POLIXENES:\n",
            "\n",
            "JULIET:\n",
            "Sons, good Lapurel, I can connourned.\n",
            "\n",
            "ISABELLA:\n",
            "I am too?\n",
            "\n",
            "CORIOLANUS:\n",
            "Then I:\n",
            "This say\n",
            "I would you devendred in Angelo, tell me that fourself\n",
            "A rasge with booth strike at this things York,\n",
            "With groans he dight her\n",
            "As thou things and tear.\n",
            "Grom her needs in his vain, love the spies gleature of you in repent?\n",
            "Be you that, and she's small die to heaven, whence wounds,\n",
            "God night intoorment, opHer hempether?\n",
            "\n",
            "Led here: thou wouldst never there?\n",
            "\n",
            "HESMIONE:\n",
            "Then,'s content doath unto the man propaint your master than I chance\n",
            "To have your demands revenged,\n",
            "To say't you humble dead chriobs with ling\n",
            "To get your love-taste no tear incosait to\n",
            "male.\n",
            "\n",
            "Second Murderer:\n",
            "I speak begin friends.\n",
            "\n",
            "MERCUTIO:\n",
            "Master's fair erraces, I will gentle wondr'd,\n",
            "I know, my love, I am, I go, I'll hence, upon my father,\n",
            "His thrict's the summer day.'\n",
            "\n",
            "CAPULET:\n",
            "Ay, madam, for this I would not do to seed\n",
            "And them more virwin to my well ware.\n",
            "\n",
            "MERCUTIO:\n",
            "Come, louch'd,\n",
            "He might the case from dianly. O que, you'll give me the ord.\n",
            "\n",
            "BRUKENBURY:\n",
            "Let him forband him vialate; make you labour's heid,\n",
            "And thence will brother that their learn'd in great too:\n",
            "For, that I have dance thee stir us an imager: I fear\n",
            "Aid ever to tempty hours looks alain.\n",
            "\n",
            "GLOUCESTER:\n",
            "I glay so we?\n",
            "Tell me our comal make the lady as my held:\n",
            "If you arr medd?\n",
            "\n",
            "KING HENRY VI:\n",
            "Very wallik! Blow, you have I thy right of them;\n",
            "And yet not in this evilst wife that dint:\n",
            "Best, see the poptern of such a rais\n",
            "To fear it as your fury: that you dought your gentleo;\n",
            "And most almost as our soul\n",
            "Could rit no eyes wrave forswear their lives,'d hand lived in the vault,\n",
            "Or, I'll go with youtheable, that was cheat enjoying\n",
            "My love is crave? strike a dise because.\n",
            "\n",
            "COMINIUS:\n",
            "I saw you:\n",
            "I pray you the wind of war, provoked reap.\n",
            "Qut who breathe rest you there to be\n",
            "Detil as this no abide sworn\n",
            "Lord 'Formed wheney and be. Hark, farewell,\n",
            "Her outh the business, to-day ven the mistress wan us,\n",
            "it may practised answitedly: I have forsented kings.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "Well, be not I; Lew me he his oath doing,\n",
            "To-mother hold have TREMIO:\n",
            "Why, is he boney'd ride, came met your banishment.\n",
            "\n",
            "Mosthy market have at off?\n",
            "\n",
            "CLARENCE:\n",
            "My lord, no, the matters here in everioanted mems\n",
            "By heaven that I widow are mercy;\n",
            "Or this dunger faced and brish\n",
            "To lovour gentleman.\n",
            "\n",
            "CAPULET:\n",
            "For I have field and your counsel to thee?\n",
            "If thou wert grace to her? O, must do That an\n",
            "Good majesty here; to me i\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsimgaIDG0-r"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}